{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sK6Uuoy5qsua",
    "outputId": "3a168853-c3d8-4bb3-9906-a2ea6af55e22"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLUhtllmz_kL"
   },
   "outputs": [],
   "source": [
    "#/content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9w8HifRHR1_a"
   },
   "outputs": [],
   "source": [
    "project_id = 'tweetprediction'\n",
    "bucket_name = 'ticks_with_indicators_with_volume'\n",
    "bucket_name_tick_data = 'bittrex_tick_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTJ7bYMtHmvS"
   },
   "outputs": [],
   "source": [
    "#from google.colab import auth\n",
    "#auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIeTggmwJlZk",
    "outputId": "e7c3252a-3db1-4115-a88d-04dcb156da2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#   import talib\n",
    "# except:\n",
    "#   #!cp /content/drive/MyDrive/Privat/Crypto/ta-lib-0.4.0-src.tar.gz ta-lib-0.4.0-src.tar.gz\n",
    "#   !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
    "#   !tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
    "#   # !unzip /content/drive/MyDrive/Privat/Crypto/talibmaked.zip -d /\n",
    "#   %cd ta-lib\n",
    "#   !./configure --prefix=/usr\n",
    "#   !make\n",
    "#   !make install\n",
    "#   !pip install Ta-Lib\n",
    "  \n",
    "#   import talib\n",
    "#   from talib import MA_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5lm4kZqsHl6"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add custom import paths for DataStreamCreator and IndicatorCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "current_dir_splitted = current_dir.split(os.sep)\n",
    "current_dir_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the import directories for the DataStreamCreator and the IndicatorCalculator\n",
    "dsc_dir = os.path.join(os.sep, *current_dir_splitted[:-2], 'DataStreamCreator')\n",
    "print(f\"dsc_dir: {dsc_dir}\")\n",
    "\n",
    "ind_dir = os.path.join(os.sep, *current_dir_splitted[:-2], 'IndicatorCalculator')\n",
    "print(f\"ind_dir: {ind_dir}\")\n",
    "\n",
    "# Add them to the import paths\n",
    "sys.path.insert(0, dsc_dir)\n",
    "sys.path.insert(0, ind_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-9z-QVIqwcZ"
   },
   "outputs": [],
   "source": [
    "# Import the actual classes\n",
    "from IndicatorCalculator import IndicatorCalculator\n",
    "import DataStreamCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOd6PecsLwaG",
    "outputId": "cb3491d8-b051-4da9-d3f4-c7fa7d3e5029"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYePtDVpqtkN"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, ReLU, Add, Flatten, Concatenate, LayerNormalization, UpSampling2D, Activation, LSTM, Multiply, Dropout, Reshape, Permute, BatchNormalization, MaxPooling1D, AveragePooling1D, MaxPooling3D, AveragePooling2D, LayerNormalization, MaxPooling2D\n",
    "# from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define all the parameters and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data path\n",
    "DATA_PATH = os.path.join(os.sep, *current_dir_splitted[:-2], 'DemoData')\n",
    "print(f\"DATA_PATH: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfq5Vbzq_sxh"
   },
   "outputs": [],
   "source": [
    "# Define a global random seed\n",
    "RANDOM_SEED = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtDdNX7JsJ7X"
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = \"gs://ticks_with_indicators_with_volume/chk/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fcaN9Sxq0gO",
    "outputId": "bb01e289-49e0-4be4-e6c5-d0c8ef757026"
   },
   "outputs": [],
   "source": [
    "# Load the example OHLCV file\n",
    "EXAMPLE_FILE_PATH = os.path.join(DATA_PATH, \"BTC-USDT.csv\")\n",
    "EXAMPLE_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9gBeRtnxKMD"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 64\n",
    "X_BLOCK_LENGHT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_aforzGxoSj"
   },
   "outputs": [],
   "source": [
    "DATASET_NUMBER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okMEyjv-xScF"
   },
   "outputs": [],
   "source": [
    "# Parameterset 7\n",
    "PARAM_SET = 7\n",
    "\n",
    "Y_TYPE = 3 # 0 for categorical, 1 for float, 2 for gain lookaround, 3 for entry/exit\n",
    "SMCNT = 48\n",
    "SMCNT2 = 48\n",
    "\n",
    "EXPECTED_GAIN_LOOKFORWARD = 1*24\n",
    "ENTR_THR = 0.9\n",
    "ENTR_THR2 = 0.8\n",
    "ENTR_THR3 = 0.0\n",
    "EXIT_THR = -0.5\n",
    "EXIT_THR2 = 0.1\n",
    "\n",
    "Y_LOOKAHEAD_CNT = 1\n",
    "GAIN_LOOKAROUND_CNT = 1\n",
    "\n",
    "CLASS_WEIGHT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JVT1Z2U0lW8"
   },
   "outputs": [],
   "source": [
    "SHORTSPAN = 6\n",
    "MIDSPAN = 48\n",
    "LONGSPAN = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKkv21lAxYEP"
   },
   "outputs": [],
   "source": [
    "BOS_TOKEN_ID = 255\n",
    "PAD_TOKEN_ID = 255\n",
    "EOS_TOKEN_ID = 255\n",
    "MASK_TOKEN_ID = 255\n",
    "\n",
    "MASK_FACTOR = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sQ6q_gjxqV5"
   },
   "outputs": [],
   "source": [
    "# Set the seed for token masking random\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvZfMFeIsPA4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WXzGlS6Cwn9"
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckiy623zBJ8H"
   },
   "source": [
    "# Demonstration of XBlockGenerator\n",
    "\n",
    "The `XBlockGenerator` class is used to generate time-frame slices (= X-blocks) out of a time series table of tick and indicator data.\n",
    "For this task, an input `pd.DataFrame` `tickAndIndicatorDF` is processed row by row.\n",
    "It is called 'X' because its purpose is to be used as input data for machine learning networks (== X-data).\n",
    "\n",
    "Every X-block is created by using a specific amount of table rows, defined by the `int` parameter `X_Block_lenght`.\n",
    "If the tick data is in hours and `X_lookback_cnt=12`, the generator would return slices of 12 hours.\n",
    "The step size between the X-blocks is 1, so the resulting DFs in the example would be: 00:00-11:00, 01:00-12:00, 02:00-13:00, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84UDRtoLZcja"
   },
   "source": [
    "For demonstration purposes, only a OHLCV dataframe without indicators is used, to keep the confusion low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "k91JXgPSCzc-",
    "outputId": "9b12ab02-ccee-49ca-ec94-54d0b29e2191"
   },
   "outputs": [],
   "source": [
    "# Load the OHLCV table\n",
    "tickdata = pd.read_csv(EXAMPLE_FILE_PATH)\n",
    "tickdata.set_index(\"startsAt\", inplace = True) # Todo: Take care of the index name\n",
    "tickdata.sort_index()\n",
    " \n",
    "# Crop the lookback\n",
    "# Todo: Describe why cropping is necessary\n",
    "# tickdata = tickdata.iloc[X_BLOCK_LENGHT:]\n",
    "\n",
    "tickdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the open column of the data set\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(tickdata.loc[:,'open'], color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2ZUJNZxBJp8",
    "outputId": "68ac6847-a77f-474e-e07f-d1757efd424c"
   },
   "outputs": [],
   "source": [
    "# Generate 2 X-Blocks with a size of 100 timesteps\n",
    "xBlockGenerator = DataStreamCreator.XBlockGenerator(tick_and_indicator_DF=tickdata,\n",
    "                                                    generator_batch_size=2,\n",
    "                                                    X_Block_lenght=100\n",
    "                                                    )\n",
    "xBlocks = next(xBlockGenerator)\n",
    "print(f\"tickdata.shape: {tickdata.shape}\")\n",
    "print(f\"xBlocks.shape: {xBlocks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Ci-jSPbHAQ9",
    "outputId": "652235b7-66dd-42e2-d2df-6d65e00ea29d"
   },
   "outputs": [],
   "source": [
    "# The generated X-blocks do not have column names any more, as they are just an array\n",
    "# Plot the first feature index of the dataset (which is the open price)\n",
    "# The gray line is the 'whole' dataset\n",
    "# The red one is the slice of the X-Block\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(tickdata.loc[:,'open'].values[:500], color=\"lightgray\")\n",
    "\n",
    "# Plot the first block\n",
    "ax1.plot(xBlocks[0,:,0], color=\"red\", linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "c9MqyHY8ZuYN",
    "outputId": "5dbf7706-ba81-420d-dabb-8799fbf11541"
   },
   "outputs": [],
   "source": [
    "# Here, the second block is plotted. It is shifted against the \"base data\", as\n",
    "# it's first datapoint is at timestamp 1. But it also reached one more timestamp\n",
    "# into the future.\n",
    "# Shifting is because X-blocks are simple array without \"global\" time information,\n",
    "# as they are just an array\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(tickdata.loc[:,'open'].values[:200], color=\"lightgray\")\n",
    "\n",
    "# Plot the first block\n",
    "ax1.plot(xBlocks[0,:,0], color=\"red\", linewidth = 4)\n",
    "\n",
    "# Plot the second block\n",
    "ax1.plot(xBlocks[1,:,0], color=\"green\", linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymYjInM5bgf3",
    "outputId": "5bf1ce06-85a8-4999-bfb4-b59ac8266cc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate X-Blocks until there is no more data to get the block\n",
    "# at the end of the dataset\n",
    "\n",
    "latest_blocks = None\n",
    "\n",
    "while True:\n",
    "  try:\n",
    "    latest_blocks = next(xBlockGenerator)\n",
    "  except StopIteration:\n",
    "    # Exit if there is no more data\n",
    "    break\n",
    "latest_blocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "evVB2iV3b4Lb",
    "outputId": "b57ab76d-0cc8-4dc7-c46d-f6de4ad77ce5"
   },
   "outputs": [],
   "source": [
    "# Plot the latest data\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "ax1.plot(latest_blocks[-1,:,0], color=\"red\", linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "oPFOgRuEcIRg",
    "outputId": "7b656891-77ca-4b15-db6a-45a4b8080f79"
   },
   "outputs": [],
   "source": [
    "# Plot the end of the data table\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(tickdata.loc[:,'open'].values[-500:], color=\"lightgray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of YDataGenerator\n",
    "\n",
    "The `YDataGenerator` class is used to generate future information for training out of a time series table of tick and indicator data.\n",
    "This can for example be the assets relative price or gain in 24 hours, the direction of movement, or trade entry and exit signals.\n",
    "\n",
    "IMPORTANT: This class can of course not look into the future, so if you want to output the price in 24 hours, this can only be done until\n",
    "table index `len(timestamp rows) - 24`, otherwise it would really have to look into the future!\n",
    "\n",
    "The purpose of this class is to generate machine-learning target data (y-values) according to the X-Blocks generated by the `XBlockGenerator`.\n",
    "The X-Block contains data from the past, which is known in a live application, while the Y data is unknown in the live application, and therefore has to be predicted.\n",
    "Here, it can be generated for training purposes using historical data.\n",
    "\n",
    "As a price basis for calculating the y data, the `open` column of the input table `tick_DF` is used.\n",
    "\n",
    "There are 4 different types of y data available, which are shown each on its own in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-J7-V5YbFzH"
   },
   "source": [
    "---\n",
    "### Visualization of y for Y_DATA_TYPE_DIRECTION_FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the template parameter dict for this y type\n",
    "Y_PARAM_DICT_TEMPLATE = DataStreamCreator.YDataGenerator.PARAM_DICT_TEMPLATE_Y_DATA_TYPE_DIRECTION_FLOAT\n",
    "Y_PARAM_DICT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDataGenerator = DataStreamCreator.YDataGenerator(tick_DF=tickdata,\n",
    "                                                    generator_batch_size=2,\n",
    "                                                    lookback_cnt=100,\n",
    "                                                    y_type_dict=Y_PARAM_DICT_TEMPLATE\n",
    "                                                    )\n",
    "yData = next(yBlockGenerator)\n",
    "print(f\"yData.shape: {yData.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "rNMB8gPwbK9J",
    "outputId": "8e84b7de-84d5-49ed-a519-4170bfe1acde"
   },
   "outputs": [],
   "source": [
    "y_dir = None\n",
    "y_dir2nd = None\n",
    "\n",
    "ite = DataStreamCreator.FileListToDataStream([testfiles[VISUAL_ID]], 31, TEST_PATH,\n",
    "                                              smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                              X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                              y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                              gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                              shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                              y_type = Y_TYPE,\n",
    "                                              expected_gain_lookforward=EXPECTED_GAIN_LOOKFORWARD, entr_thr=ENTR_THR, entr_thr2=ENTR_THR2, entr_thr3=ENTR_THR3,\n",
    "                                              exit_thr=EXIT_THR, exit_thr2=EXIT_THR2,\n",
    "                                              parallel_generators = 1, random_seed=RANDOM_SEED,\n",
    "                                              y_exponent = 1.0,\n",
    "                                              shuffle = False)\n",
    "while True:\n",
    "  try:    \n",
    "    ne = next(ite)\n",
    "  except StopIteration:\n",
    "    break\n",
    "  if y_dir is None or y_dir2nd is None:\n",
    "    y_dir = ne[1][:,0]\n",
    "    y_dir2nd = ne[1][:,1]\n",
    "  else:\n",
    "    y_dir = np.concatenate([y_dir, ne[1][:,0]])\n",
    "    y_dir2nd = np.concatenate([y_dir2nd, ne[1][:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVJ5b3pycLwH"
   },
   "outputs": [],
   "source": [
    "# Load the tick table\n",
    "tickpath = os.path.join(TEST_PATH, testfiles[VISUAL_ID])\n",
    "tickdata = pd.read_csv(tickpath)\n",
    "tickdata.set_index(\"startsAt\", inplace = True)\n",
    "tickdata.sort_index()\n",
    " \n",
    "# Crop the lookback\n",
    "tickdata = tickdata.iloc[X_LOOKBACK_CNT:]\n",
    "\n",
    "tickdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky3xgVHGOMGr"
   },
   "outputs": [],
   "source": [
    "y_dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wce8gu8obf_W"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(y_dir2nd, color=\"red\")\n",
    "ax1.plot(y_dir, color=\"green\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "exit_naned = copy.deepcopy(y_dir2nd)\n",
    "exit_naned[exit_naned == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "\n",
    "ax1.set_ylim(-1.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfCxbY6KVXrp"
   },
   "source": [
    "# Visualization of y for Y_DATA_TYPE_DIRECTION_CATEGORICAL\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kAyTmdCVXrr"
   },
   "outputs": [],
   "source": [
    "VISUAL_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM-sp3-uVXrt"
   },
   "outputs": [],
   "source": [
    "Y_TYPE = DataStreamCreator.YDataGenerator.Y_DATA_TYPE_DIRECTION_CATEGORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lca98CJvtwWb"
   },
   "outputs": [],
   "source": [
    "GAIN_LOOKAROUND_CNT = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVwtQ51GVXru"
   },
   "outputs": [],
   "source": [
    "y_category = None\n",
    "\n",
    "ite = DataStreamCreator.FileListToDataStream([testfiles[VISUAL_ID]], 31, TEST_PATH,\n",
    "                                              smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                              X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                              y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                              gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                              shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                              y_type = Y_TYPE,\n",
    "                                              expected_gain_lookforward=EXPECTED_GAIN_LOOKFORWARD, entr_thr=ENTR_THR, entr_thr2=ENTR_THR2, entr_thr3=ENTR_THR3,\n",
    "                                              exit_thr=EXIT_THR, exit_thr2=EXIT_THR2,\n",
    "                                              parallel_generators = 1, random_seed=RANDOM_SEED,\n",
    "                                              y_exponent = 1.0,\n",
    "                                              shuffle = False)\n",
    "while True:\n",
    "  try:    \n",
    "    ne = next(ite)\n",
    "  except StopIteration:\n",
    "    break\n",
    "  if y_category is None:\n",
    "    y_category = ne[1]\n",
    "  else:\n",
    "    y_category = np.concatenate([y_category, ne[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4OKijnQVXry"
   },
   "outputs": [],
   "source": [
    "# Load the tick table\n",
    "tickpath = os.path.join(TEST_PATH, testfiles[VISUAL_ID])\n",
    "tickdata = pd.read_csv(tickpath)\n",
    "tickdata.set_index(\"startsAt\", inplace = True)\n",
    "tickdata.sort_index()\n",
    " \n",
    "# Crop the lookback\n",
    "tickdata = tickdata.iloc[X_LOOKBACK_CNT:]\n",
    "\n",
    "tickdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3K-Ncz9VXrz"
   },
   "outputs": [],
   "source": [
    "y_category.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WwmxbdDfdXs"
   },
   "outputs": [],
   "source": [
    "# This chart shows the categories (0=falling, 1=neutral, 2=rising)\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(y_category, color=\"blue\", linewidth=3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N43qGQLqVXr0"
   },
   "outputs": [],
   "source": [
    "# This chart shows a comparison of the categories to the Y_DATA_TYPE_DIRECTION_FLOAT values\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(y_dir2nd, color=\"red\", linestyle=\":\") # Data from the previous example (Y_DATA_TYPE_DIRECTION_FLOAT)\n",
    "ax1.plot(y_dir, color=\"green\", linestyle=\":\") # Data from the previous example (Y_DATA_TYPE_DIRECTION_FLOAT)\n",
    "ax1.plot(y_category, color=\"blue\", linewidth=3)\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwNatkXOr7F7"
   },
   "source": [
    "# Visualization of y for Y_DATA_TYPE_GAIN_LOOKAROUND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kPvUosCsIbT"
   },
   "outputs": [],
   "source": [
    "VISUAL_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TQyjVUjsLjp"
   },
   "outputs": [],
   "source": [
    "Y_TYPE = 2 #DataStreamCreator.YDataGenerator.Y_DATA_TYPE_DIRECTION_CATEGORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQ-v7Vd8sPp1",
    "outputId": "1fbe17bc-019f-4290-da93-5a0de5cbf42a"
   },
   "outputs": [],
   "source": [
    "y_gains = None\n",
    "# gains[:, 0] = np.tanh(_max_past_gain_slice)\n",
    "# gains[:, 1] = np.tanh(_max_past_gain_ma_slice)\n",
    "# gains[:, 2] = np.tanh(_max_past_gain_dir_slice)\n",
    "\n",
    "# gains[:, 3] = np.tanh(_max_future_gain_slice)\n",
    "# gains[:, 4] = np.tanh(_max_future_gain_ma_slice)\n",
    "# gains[:, 5] = np.tanh(_max_future_gain_dir_slice)\n",
    "\n",
    "ite = DataStreamCreator.FileListToDataStream([testfiles[VISUAL_ID]], 31, TEST_PATH,\n",
    "                                              smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                              X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                              y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                              gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                              shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                              y_type = Y_TYPE,\n",
    "                                              expected_gain_lookforward=EXPECTED_GAIN_LOOKFORWARD, entr_thr=ENTR_THR, entr_thr2=ENTR_THR2, entr_thr3=ENTR_THR3,\n",
    "                                              exit_thr=EXIT_THR, exit_thr2=EXIT_THR2,\n",
    "                                              parallel_generators = 1, random_seed=RANDOM_SEED,\n",
    "                                              y_exponent = 1.0,\n",
    "                                              shuffle = False)\n",
    "while True:\n",
    "  try:    \n",
    "    ne = next(ite)\n",
    "  except StopIteration:\n",
    "    break\n",
    "  if y_gains is None:\n",
    "    y_gains = ne[1]\n",
    "  else:\n",
    "    y_gains = np.concatenate([y_gains, ne[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzPxD2x6viro",
    "outputId": "04005337-f614-4704-d665-098c6fd4c08c"
   },
   "outputs": [],
   "source": [
    "# Load the tick table\n",
    "tickpath = os.path.join(TEST_PATH, testfiles[VISUAL_ID])\n",
    "tickdata = pd.read_csv(tickpath)\n",
    "tickdata.set_index(\"startsAt\", inplace = True)\n",
    "tickdata.sort_index()\n",
    " \n",
    "# Crop the lookback\n",
    "tickdata = tickdata.iloc[X_LOOKBACK_CNT:]\n",
    "\n",
    "tickdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KT9ZzA6tvd7a",
    "outputId": "72d1193e-5ac0-41f3-b5c6-fe9983a00486"
   },
   "outputs": [],
   "source": [
    "# Todo important: Not the same size as the tick table!!!!\n",
    "y_gains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "7rXOW66EvgLl",
    "outputId": "142a512f-10c8-4c48-8b43-c852a2a28bcb"
   },
   "outputs": [],
   "source": [
    "# This chart shows a comparison of the categories to the Y_DATA_TYPE_DIRECTION_FLOAT values\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(y_gains[:,0], color=\"lightgray\")\n",
    "ax1.plot(y_gains[:,1], color=\"red\", label=\"Past gain\")\n",
    "ax1.plot(y_gains[:,4], color=\"green\", label=\"Future gain\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "kU1GFkSYwqiM",
    "outputId": "454370b1-310f-4bd9-caca-7414abe8ea4f"
   },
   "outputs": [],
   "source": [
    "# This chart shows a comparison of the categories to the Y_DATA_TYPE_DIRECTION_FLOAT values\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax1.plot(y_gains[:,2], color=\"red\", label=\"Past gain derviation\")\n",
    "ax1.plot(y_gains[:,5], color=\"green\", label=\"Future gain derviation\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "fphWUe4ir7ab",
    "outputId": "7f9f69a5-3ec5-4dde-8721-07018c037d88"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moBea0tTbFbX"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqEsad88KGok"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15sjJeREKGkV"
   },
   "outputs": [],
   "source": [
    "# Load the BIRCH categorizer\n",
    "with open(\"/content/drive/MyDrive/Privat/Crypto/birchNew.pkl\", 'rb') as pickle_file:\n",
    "  brc = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk0ASVs7KEmp"
   },
   "outputs": [],
   "source": [
    "def NormForBirch(XIn):\n",
    "  _THR = 100\n",
    "  normed = copy.deepcopy(XIn)\n",
    "\n",
    "  normed[normed >= _THR] = _THR\n",
    "  normed[normed <= -_THR] = -_THR\n",
    "\n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRRpcrxUf1J5"
   },
   "outputs": [],
   "source": [
    "def XFrameGenerator(oneTestFileName):\n",
    "\n",
    "  ite = DataStreamCreator.FileListToDataStream([oneTestFileName], BATCH_SIZE, TEST_PATH,\n",
    "                                                smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                                X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                                y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                                gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                                shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                                y_type = Y_TYPE,\n",
    "                                                expected_gain_lookforward=EXPECTED_GAIN_LOOKFORWARD, entr_thr=ENTR_THR, entr_thr2=ENTR_THR2, entr_thr3=ENTR_THR3,\n",
    "                                                exit_thr=EXIT_THR, exit_thr2=EXIT_THR2,\n",
    "                                                parallel_generators = 1, random_seed=RANDOM_SEED,\n",
    "                                                y_exponent = 1.0,\n",
    "                                                shuffle = False)\n",
    "  while True:\n",
    "    try:    \n",
    "      ne = next(ite)\n",
    "\n",
    "      # Xin as in the mask function\n",
    "      Xin = ne[0]\n",
    "\n",
    "      # y can be used directly\n",
    "      y = ne[1]\n",
    "\n",
    "      # Store the input shape\n",
    "      inputShape = Xin.shape\n",
    "\n",
    "      # Create a \"sentence\"\n",
    "      X_normed = NormForBirch(Xin)\n",
    "      X_normed = np.reshape(X_normed, (X_normed.shape[0] * X_normed.shape[1], X_normed.shape[2]))\n",
    "      sentence = brc.predict(X_normed)\n",
    "      sentence = np.reshape(sentence, inputShape[:2])\n",
    "\n",
    "      yield sentence\n",
    "    \n",
    "    except StopIteration:\n",
    "      break\n",
    "    \n",
    "xFrameGenDebug = XFrameGenerator(testfiles[VISUAL_ID])\n",
    "xFrameGenDebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaZrGiC5x2Lj"
   },
   "outputs": [],
   "source": [
    "ne = next(xFrameGenDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbC4AZntx30B"
   },
   "outputs": [],
   "source": [
    "ne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGcV6A1_x4u6"
   },
   "outputs": [],
   "source": [
    "ne[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E3uAgQgzYIX"
   },
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg86UwN6zbZW"
   },
   "outputs": [],
   "source": [
    "cfg = RobertaConfig()\n",
    "cfg.vocab_size = 256\n",
    "cfg.max_position_embeddings = X_LOOKBACK_CNT\n",
    "cfg.bos_token_id = BOS_TOKEN_ID\n",
    "cfg.pad_token_id = PAD_TOKEN_ID\n",
    "cfg.eos_token_id = EOS_TOKEN_ID\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efjsM80DzdfR"
   },
   "outputs": [],
   "source": [
    "#@title CreateModelRobertaEntryExit\n",
    "def CreateModelRobertaEntryExit():\n",
    "\n",
    "  # Build your model input\n",
    "  inputSentence = Input(shape=(X_LOOKBACK_CNT), name='input', dtype='int32')\n",
    "  \n",
    "  nlp = TFRobertaModel(cfg, name=\"Roberta\")(inputSentence)\n",
    "  flat = Flatten(name=\"FlattenNLP\")(nlp['last_hidden_state'])\n",
    "\n",
    "  categories = Dense(3, name=\"DenseCategories\", activation=\"softmax\")(flat)\n",
    "\n",
    "  output = Concatenate(name=\"ConcatOutput\")([categories])\n",
    "  outputs = [output]\n",
    "\n",
    "  mnamesuffix = \"_1\"\n",
    "\n",
    "  # And combine it all in a model object\n",
    "  model = Model(inputs=inputSentence, outputs=outputs, name='ModelRobertaEntryExit'+mnamesuffix)\n",
    "\n",
    "  return model\n",
    "\n",
    "model = CreateModelRobertaEntryExit()\n",
    "model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xzjWt2fzhVF"
   },
   "outputs": [],
   "source": [
    "# Set an optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05,\n",
    "    epsilon=1e-06,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSId6c8uzieV"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = tf.keras.losses.MeanSquaredError(), \n",
    "    metrics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGFJiMmrzkOc"
   },
   "outputs": [],
   "source": [
    "copy_filenames = ['gs://ticks_with_indicators_with_volume/chk/TPU_ModelRobertaEntryExit_1_128LB_256VC_FullPrediction/cp_daily_valid_27_end/model.h5'\n",
    "                  ]\n",
    "for p in copy_filenames:\n",
    "  fn = p.split(\"/\")[-1]\n",
    "  cpnt = p.split(\"/\")[-2]\n",
    "\n",
    "  os.mkdir(os.path.join(\"/content\", cpnt))\n",
    "\n",
    "  localPath = os.path.join(\"/content\", cpnt, fn)\n",
    "\n",
    "  if (\"model.h5\" in p):\n",
    "    localPathModel = localPath\n",
    "  elif (\"w.pickle\" in p):\n",
    "    localPathW = localPath\n",
    "\n",
    "  with file_io.FileIO(p, mode='rb') as input_f:\n",
    "    with file_io.FileIO(localPath, mode='wb+') as output_f:\n",
    "      output_f.write(input_f.read())\n",
    "      print(\"Pulled from bucket: '\" + fn + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvO9Lj3mzvUv"
   },
   "outputs": [],
   "source": [
    "print(f\"Loading {localPathModel}\")\n",
    "model.load_weights(localPathModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9agETIP5VU7"
   },
   "outputs": [],
   "source": [
    "xFrameGenDebug = XFrameGenerator(testfiles[VISUAL_ID]) # Todo: Return shape in gen\n",
    "p = model.predict(xFrameGenDebug)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyXHwLtbsust"
   },
   "outputs": [],
   "source": [
    "# Adapt prediction to input shape (a window in the beginning cannot be predicted, as lookback into the past is required)\n",
    "# Todo: Get shape from gen\n",
    "pFullsize = np.empty((y_dir.shape[0], p.shape[1]))\n",
    "pFullsize[:] = np.nan\n",
    "\n",
    "pFullsize[-p.shape[0]:,:] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv3MALM5sW1B"
   },
   "outputs": [],
   "source": [
    "y_dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeZG-I60sYa7"
   },
   "outputs": [],
   "source": [
    "pFullsize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEqifvsHkrO0"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "\n",
    "# ax1.plot(y_dir2nd, color=\"red\")\n",
    "ax1.plot(y_dir, color=\"gray\")\n",
    "ax1.plot(pFullsize[:,0], color=\"green\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "exit_naned = copy.deepcopy(y_dir2nd)\n",
    "exit_naned[exit_naned == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "\n",
    "ax1.set_ylim(-1.1,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QA8OBHSJsTSm"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "\n",
    "# ax1.plot(y_dir2nd, color=\"gray\")\n",
    "ax1.plot(pFullsize[:,1], color=\"red\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "exit_naned = copy.deepcopy(y_dir2nd)\n",
    "exit_naned[exit_naned == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "\n",
    "# ax1.set_ylim(-1.1,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R__CskwD7mvf"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "\n",
    "# ax1.plot(y_dir2nd, color=\"gray\")\n",
    "ax1.plot(pFullsize[:,0], color=\"green\")\n",
    "ax1.plot(pFullsize[:,1], color=\"red\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "exit_naned = copy.deepcopy(y_dir2nd)\n",
    "exit_naned[exit_naned == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "\n",
    "ax1.set_ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qU1orcFwu7D"
   },
   "outputs": [],
   "source": [
    "ENTRY_THR = 0.4\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "\n",
    "# ax1.plot(y_dir2nd, color=\"red\")\n",
    "# ax1.plot(y_dir, color=\"gray\")\n",
    "ax1.plot(pFullsize[:,0], color=\"gray\")\n",
    "\n",
    "# ax1.set_xlim(0,1000)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "entries = np.array([pFullsize[:,0] >= ENTRY_THR])[0,:] * tickdata.loc[:,\"close\"].values\n",
    "entries[entries == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "ax2.plot(entries, color = \"green\", marker=\"X\", markersize=10)\n",
    "\n",
    "ax1.set_ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWy9xmBOyKpJ"
   },
   "outputs": [],
   "source": [
    "EXIT_THR = 0.4\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "\n",
    "ax1.plot(pFullsize[:,1], color=\"gray\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "entries = np.array([pFullsize[:,1] >= EXIT_THR])[0,:] * tickdata.loc[:,\"close\"].values\n",
    "entries[entries == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "ax2.plot(entries, color = \"red\", marker=\"X\", markersize=10)\n",
    "\n",
    "ax1.set_ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfk7ZeFZyVdA"
   },
   "outputs": [],
   "source": [
    "ENTRY_THR2 = 0.5\n",
    "ENTRY_THR2 = 0.0\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "\n",
    "ax1.plot(pFullsize[:,1], color=\"gray\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "y_dir_naned = copy.deepcopy(y_dir)\n",
    "y_dir_naned[y_dir_naned == 0] = np.nan\n",
    "\n",
    "entries = np.array((pFullsize[:,0] >= ENTRY_THR) & (pFullsize[:,1] <= ENTRY_THR2)) * tickdata.loc[:,\"close\"].values\n",
    "entries[entries == 0] = np.nan\n",
    "\n",
    "ax2.plot(tickdata.loc[:,\"close\"].values, color = \"black\")\n",
    "ax2.plot(entries, color = \"green\", marker=\"X\", markersize=10)\n",
    "\n",
    "ax1.set_ylim(-1.1,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrYcvx-Lx63e"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL2h_bVUyODm"
   },
   "source": [
    "# Generate the X-Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQrl3BMqybw0"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(testfiles))):\n",
    "  # Load the tick table\n",
    "  tickpath = os.path.join(TEST_PATH, testfiles[i])\n",
    "  tickdata = pd.read_csv(tickpath)\n",
    "  tickdata.set_index(\"startsAt\", inplace = True)\n",
    "  tickdata.sort_index()\n",
    "\n",
    "  # Print the reuqired batches as info\n",
    "  required_batches = int(np.floor((tickdata.shape[0]-X_LOOKBACK_CNT) / BATCH_SIZE))\n",
    "  print(f\"Required batches: {required_batches}\")\n",
    "\n",
    "  # Create a X frame generator (Todo: the csv is loaded again)\n",
    "  xFrameGen = XFrameGenerator(testfiles[i])\n",
    "\n",
    "  # Generate all frame\n",
    "  n = 0\n",
    "  xFrames = None\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      print(f\"Batch {n} of {required_batches}\")\n",
    "      n += 1\n",
    "\n",
    "      ne = next(xFrameGen)\n",
    "      if xFrames is None:\n",
    "        xFrames = ne\n",
    "      else:\n",
    "        xFrames = np.concatenate([xFrames, ne], axis=0)\n",
    "\n",
    "    except StopIteration:\n",
    "      break\n",
    "\n",
    "  xFramePath = tickpath.replace(\".csv\", \"_xFrames.npy\")\n",
    "  np.save(xFramePath, xFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDmKLrbszX11"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYQ2HFATz1tA"
   },
   "source": [
    "# Predict on the GPU\n",
    "# Todo: Load xFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjfgaQSN0KVg"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(TEST_PATH, \"predicted\")):\n",
    "  os.mkdir(os.path.join(TEST_PATH, \"predicted\"))\n",
    "\n",
    "SAVE_PATH = os.path.join(TEST_PATH, \"predicted\", copy_filenames[0].split('/')[-3] + \"_\" + copy_filenames[0].split('/')[-2])\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "  os.mkdir(SAVE_PATH)\n",
    "\n",
    "SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "warj-Xh70DBz"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BItk2flTe-Gw"
   },
   "outputs": [],
   "source": [
    "# Get test file names - npy files\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "testfiles = [f for f in listdir(TEST_PATH) if isfile(join(TEST_PATH, f)) and \".npy\" in f ]\n",
    "testfiles = sorted(testfiles)\n",
    "len(testfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjG7pRuTyQO9"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(testfiles))):\n",
    "  print(testfiles[i])\n",
    "\n",
    "  # Load the tick table\n",
    "  tickpath = os.path.join(TEST_PATH, testfiles[i].replace(\"_xFrames.npy\", \".csv\"))\n",
    "  tickdata = pd.read_csv(tickpath)\n",
    "  tickdata.set_index(\"startsAt\", inplace = True)\n",
    "  tickdata.sort_index()\n",
    "\n",
    "  # Load the xFrame\n",
    "  xFrames = np.load(os.path.join(TEST_PATH, testfiles[i]))\n",
    "\n",
    "  # # Check time dimension\n",
    "  # if (tickdata.shape[0]-X_LOOKBACK_CNT) != xFrames.shape[0]:\n",
    "  #   print(f\"(tickdata.shape[0]-X_LOOKBACK_CNT) != xFrames.shape[0] for {testfiles[i]}\")\n",
    "  #   print((\"tickdata.shape[0]\", tickdata.shape[0]))\n",
    "  #   print((\"xFrames.shape[0]\", xFrames.shape[0]))\n",
    "  #   continue\n",
    "\n",
    "  # Predict\n",
    "  p = model.predict(xFrames)\n",
    "  print(p.shape)\n",
    "\n",
    "  # Adapt prediction to input shape (a window in the beginning cannot be predicted, as lookback into the past is required)\n",
    "  pFullsize = np.empty((tickdata.shape[0], p.shape[1]))\n",
    "  pFullsize[:] = np.nan\n",
    "  pFullsize[-p.shape[0]:,:] = p\n",
    "\n",
    "  tickdata['p_dir'] = pFullsize[:,0]\n",
    "  tickdata['p_dir2nd'] = pFullsize[:,1]\n",
    "\n",
    "  # Drop nan rows\n",
    "  tickdata.dropna(inplace = True)\n",
    "\n",
    "  # Save it\n",
    "  tickdata.to_csv(os.path.join(SAVE_PATH, testfiles[i].replace(\"_xFrames.npy\", \".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp27QHwxjtls"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn5zpFday4O2"
   },
   "source": [
    "# Predict all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VD8itUZ4e433"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  for i in tqdm(range(2,len(testfiles))):\n",
    "    # Load the tick table\n",
    "    tickpath = os.path.join(TEST_PATH, testfiles[i])\n",
    "    tickdata = pd.read_csv(tickpath)\n",
    "    tickdata.set_index(\"startsAt\", inplace = True)\n",
    "    tickdata.sort_index()\n",
    "\n",
    "    # Print the reuqired batches as info\n",
    "    required_batches = int(np.floor((tickdata.shape[0]-X_LOOKBACK_CNT) / BATCH_SIZE))\n",
    "    print(f\"Required batches: {required_batches}\")\n",
    "  \n",
    "    # Create a X frame generator (Todo: the csv is loaded again)\n",
    "    xFrameGen = XFrameGenerator(testfiles[i])\n",
    "    p = model.predict(xFrameGen)\n",
    "    print(p.shape)\n",
    "\n",
    "    # Adapt prediction to input shape (a window in the beginning cannot be predicted, as lookback into the past is required)\n",
    "    pFullsize = np.empty((tickdata.shape[0], p.shape[1]))\n",
    "    pFullsize[:] = np.nan\n",
    "    pFullsize[-p.shape[0]:,:] = p\n",
    "\n",
    "    tickdata['p_dir'] = pFullsize[:,0]\n",
    "    tickdata['p_dir2nd'] = pFullsize[:,1]\n",
    "\n",
    "    # Drop nan rows\n",
    "    tickdata.dropna(inplace = True)\n",
    "\n",
    "    # Save it\n",
    "    tickdata.to_csv(os.path.join(SAVE_PATH, testfiles[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bCQsDFGzIpp"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  for i in tqdm(range(2,len(testfiles))):\n",
    "    # Load the tick table\n",
    "    tickpath = os.path.join(TEST_PATH, testfiles[i])\n",
    "    tickdata = pd.read_csv(tickpath)\n",
    "    tickdata.set_index(\"startsAt\", inplace = True)\n",
    "    tickdata.sort_index()\n",
    "\n",
    "    # Print the reuqired batches as info\n",
    "    required_batches = int(np.floor((tickdata.shape[0]-X_LOOKBACK_CNT) / BATCH_SIZE))\n",
    "    print(f\"Required batches: {required_batches}\")\n",
    "  \n",
    "    # Create a X frame generator (Todo: the csv is loaded again)\n",
    "    xFrameGen = XFrameGenerator(testfiles[i])\n",
    "    p = model.predict(xFrameGen)\n",
    "    print(p.shape)\n",
    "\n",
    "    # Adapt prediction to input shape (a window in the beginning cannot be predicted, as lookback into the past is required)\n",
    "    pFullsize = np.empty((tickdata.shape[0], p.shape[1]))\n",
    "    pFullsize[:] = np.nan\n",
    "    pFullsize[-p.shape[0]:,:] = p\n",
    "\n",
    "    tickdata['p_dir'] = pFullsize[:,0]\n",
    "    tickdata['p_dir2nd'] = pFullsize[:,1]\n",
    "\n",
    "    # Drop nan rows\n",
    "    tickdata.dropna(inplace = True)\n",
    "\n",
    "    # Save it\n",
    "    tickdata.to_csv(os.path.join(SAVE_PATH, testfiles[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziS7-tXmy8uU"
   },
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDYNHafLy-oM"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMEInG2FnqyV"
   },
   "outputs": [],
   "source": [
    "def TransformXFrameToSentenceMaskLatest(Xin, hideNonMaskedTokensInY = True):\n",
    "\n",
    "  total_item_count = BATCH_SIZE * X_LOOKBACK_CNT\n",
    "\n",
    "  # Store the input shape\n",
    "  inputShape = Xin.shape\n",
    "\n",
    "  # Create a \"sentence\"\n",
    "  X_normed = NormForBirch(Xin)\n",
    "  X_normed = np.reshape(X_normed, (X_normed.shape[0] * X_normed.shape[1], X_normed.shape[2]))\n",
    "  sentence = brc.predict(X_normed)\n",
    "  sentence = np.reshape(sentence, inputShape[:2])\n",
    "\n",
    "  # Mask it\n",
    "  sentenceMasked = copy.deepcopy(sentence) #np.array(np.reshape(sentence, (sentence.shape[0] * sentence.shape[1])))\n",
    "  sentenceMasked[:,-1] = MASK_TOKEN_ID\n",
    "\n",
    "  # Replace control tokens\n",
    "  sentenceMasked[sentenceMasked == BOS_TOKEN_ID] = MASK_TOKEN_ID\n",
    "  sentenceMasked[sentenceMasked == PAD_TOKEN_ID] = MASK_TOKEN_ID\n",
    "  sentenceMasked[sentenceMasked == EOS_TOKEN_ID] = MASK_TOKEN_ID\n",
    "\n",
    "  # sentenceMasked = np.reshape(sentenceMasked, sentence.shape)\n",
    "\n",
    "  # Replace all token positions in y where no mask is in X with -1 \n",
    "  if hideNonMaskedTokensInY:\n",
    "    sentence[MASK_TOKEN_ID != sentenceMasked] = -1\n",
    "\n",
    "  return (sentence,sentenceMasked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TBE84-7oj_I"
   },
   "source": [
    "# Generate an array for back-translating vocab intergers to chart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijC6Qw5e34IC"
   },
   "outputs": [],
   "source": [
    "stop\n",
    "can be skipped to next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSgGjqJXoveS"
   },
   "outputs": [],
   "source": [
    "def backTranslateGenerator():\n",
    "  it = DataStreamCreator.FileListToDataStream(trainfiles, BATCH_SIZE, TRAIN_PATH,\n",
    "                                              smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                              X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                              y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                              gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                              shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                              y_type = Y_TYPE,\n",
    "                                              parallel_generators = 32, random_seed=RANDOM_SEED,\n",
    "                                              y_exponent = 1.0,\n",
    "                                              norm_price_related_indicators = False,\n",
    "                                              shuffle=True)\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      ne = next(it)\n",
    "      sentence, sentenceMasked = TransformXFrameToSentenceMaskLatest(ne[0], hideNonMaskedTokensInY = False)\n",
    "      \n",
    "      yield (sentenceMasked, sentence, ne)\n",
    "\n",
    "    except StopIteration as si:\n",
    "      logging.warning(\"StopIteration in FileListToDataStream\")\n",
    "      logging.warning(si)\n",
    "      return\n",
    "\n",
    "btg = backTranslateGenerator()\n",
    "sentenceMasked, sentence, (XData, yData)  = next(btg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xEL5kUErq8h"
   },
   "outputs": [],
   "source": [
    "btg = backTranslateGenerator()\n",
    "\n",
    "XFrameDict = {}\n",
    "for i in range(256):\n",
    "  XFrameDict[i] = []\n",
    "\n",
    "for step in tqdm(range(200)):\n",
    "  sentenceMasked, sentence, (XData, yData)  = next(btg)\n",
    "  for s in range(sentence.shape[0]):\n",
    "    # print((\"s\", s))\n",
    "\n",
    "    for wordIndex in range(sentence.shape[1]):\n",
    "      # print((\"wordIndex\"), wordIndex)\n",
    "\n",
    "      word = sentence[s, wordIndex]\n",
    "\n",
    "      # print(word)\n",
    "\n",
    "      XFrame = XData[s, wordIndex, :]\n",
    "\n",
    "      XFrameDict[word].append(XFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDmv3A3RsnRl"
   },
   "outputs": [],
   "source": [
    "for k in XFrameDict.keys():\n",
    "  print((k, len(XFrameDict[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sjhiJN9u3RL"
   },
   "outputs": [],
   "source": [
    "np.max(list(XFrameDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35qNsHJKvFni"
   },
   "outputs": [],
   "source": [
    "XFrameDict[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqoLZhxMurcq"
   },
   "outputs": [],
   "source": [
    "# Calculate the median X frame for every \"word\"/token\n",
    "XFrames = np.zeros((\n",
    "    np.max(list(XFrameDict.keys()))+1,\n",
    "    XFrameDict[0][0].shape[0]\n",
    "    ))\n",
    "\n",
    "for k in XFrameDict.keys():\n",
    "  XFrames[k,:] = np.clip(np.mean(np.array(XFrameDict[k]), axis=0), -100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7N1wKBDXa1kF"
   },
   "outputs": [],
   "source": [
    "XFrames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9newKpxvUMc"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "_ = ax1.plot(XFrames.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NcmAXL_a0Bx"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "_ = ax1.plot(XFrames[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0czQLyRwqn-"
   },
   "outputs": [],
   "source": [
    "todo check if really intended to be saved\n",
    "np.save(\"/content/drive/MyDrive/Privat/Crypto/XFrames256.npy\", XFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaaQEF-SbEJ_"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZaWazh6osch"
   },
   "source": [
    "# Predict stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qg8zZ9an24kR"
   },
   "outputs": [],
   "source": [
    "#@title dummyTrainMLM - DISABLED\n",
    "# def dummyTrainMLM():\n",
    "#   it = DataStreamCreator.FileListToDataStream(trainfiles, BATCH_SIZE, TRAIN_PATH,\n",
    "#                                               smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "#                                               X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "#                                               y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "#                                               gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "#                                               shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "#                                               y_type = Y_TYPE,\n",
    "#                                               parallel_generators = 8, random_seed=RANDOM_SEED,\n",
    "#                                               y_exponent = 1.0,\n",
    "#                                               norm_price_related_indicators = False,\n",
    "#                                               shuffle=True)\n",
    "\n",
    "#   while True:\n",
    "#     try:\n",
    "#       XOriginal, yOriginal = next(it)\n",
    "#       # sentence, sentenceMasked = TransformXFrameToMaskedSentence(ne[0])\n",
    "#       sentence, sentenceMasked = TransformXFrameToSentenceMaskLatest(XOriginal)\n",
    "\n",
    "#       yield sentenceMasked, sentence, (XOriginal, yOriginal)\n",
    "\n",
    "#     except StopIteration as si:\n",
    "#       logging.warning(\"StopIteration in FileListToDataStream\")\n",
    "#       logging.warning(si)\n",
    "#       return\n",
    "\n",
    "# tfgenTrainMLM = tf.data.Dataset.from_generator(dummyTrainMLM, output_types = (tf.int64, tf.int64), output_shapes=((BATCH_SIZE,X_LOOKBACK_CNT), (BATCH_SIZE,X_LOOKBACK_CNT)))\n",
    "# tfgenTrainMLM = tfgenTrainMLM.prefetch(tf.data.AUTOTUNE)\n",
    "# tfgenTrainMLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6QaZj3MxNZx"
   },
   "outputs": [],
   "source": [
    "def dummyTestMLM():\n",
    "  it = DataStreamCreator.FileListToDataStream(testfiles, BATCH_SIZE, TEST_PATH,\n",
    "                                              smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                              X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                              y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                              gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                              shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                              y_type = Y_TYPE,\n",
    "                                              parallel_generators = 4, random_seed=RANDOM_SEED,\n",
    "                                              y_exponent = 1.0,\n",
    "                                              shuffle=False)\n",
    " \n",
    "  total_item_count = BATCH_SIZE * X_LOOKBACK_CNT\n",
    "  items_to_mask_count = int(np.round(total_item_count * MASK_FACTOR))\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      XOriginal, yOriginal = next(it)\n",
    "      # sentence, sentenceMasked = TransformXFrameToMaskedSentence(ne[0])\n",
    "      sentence, sentenceMasked = TransformXFrameToSentenceMaskLatest(XOriginal, hideNonMaskedTokensInY = False)\n",
    "\n",
    "      yield sentenceMasked, sentence, (XOriginal, yOriginal)\n",
    "\n",
    "    except StopIteration as si:\n",
    "      logging.warning(\"StopIteration in FileListToDataStream\")\n",
    "      logging.warning(si)\n",
    "      return\n",
    "  \n",
    "tfgenTestMLM = dummyTestMLM()\n",
    "# tfgenTestMLM = tf.data.Dataset.from_generator(dummyTestMLM, output_types = (tf.int32, tf.int32), output_shapes=((BATCH_SIZE,X_LOOKBACK_CNT), (BATCH_SIZE,X_LOOKBACK_CNT)))\n",
    "# tfgenTestMLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7Dzi5XNy2uk"
   },
   "outputs": [],
   "source": [
    "wurst = next(tfgenTestMLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkfZ5uDOy5CG"
   },
   "outputs": [],
   "source": [
    "wurst[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1_gHZoi16z0"
   },
   "outputs": [],
   "source": [
    "wurst[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0M-5cnhry8kZ"
   },
   "outputs": [],
   "source": [
    "wurst[0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-EA6PyG199Y"
   },
   "outputs": [],
   "source": [
    "wurst[1][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huyZgwwqnftz"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(wurst[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V1gTRpwWvV_"
   },
   "outputs": [],
   "source": [
    "cfg = RobertaConfig()\n",
    "cfg.vocab_size = VOCAB_SIZE\n",
    "cfg.max_position_embeddings = X_LOOKBACK_CNT\n",
    "cfg.bos_token_id = BOS_TOKEN_ID\n",
    "cfg.pad_token_id = PAD_TOKEN_ID\n",
    "cfg.eos_token_id = EOS_TOKEN_ID\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LE3ZxhjDuZmA"
   },
   "outputs": [],
   "source": [
    "#@title CreateModelRobertaMLMTraining\n",
    "def CreateModelRobertaMLMTraining():\n",
    "\n",
    "  # Build your model input\n",
    "  inputSentence = Input(shape=(X_LOOKBACK_CNT), name='input', dtype='int32')\n",
    "  \n",
    "  nlp = TFRobertaModel(cfg)(inputSentence)\n",
    "\n",
    "  categories = Dense(1024, activation='softmax', name=\"Categories\")(nlp['last_hidden_state'])\n",
    "\n",
    "  outputs = [categories]\n",
    "\n",
    "  mnamesuffix = \"_2\"\n",
    "\n",
    "  # And combine it all in a model object\n",
    "  model = Model(inputs=inputSentence, outputs=outputs, name='ModelRobertaMLMTraining'+mnamesuffix)\n",
    "\n",
    "  return model\n",
    "\n",
    "# model = CreateModelRobertaMLMTraining()\n",
    "# model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xba6JYyjO07"
   },
   "outputs": [],
   "source": [
    "#@title CreateModelRobertaMLMTrainingDropout\n",
    "def CreateModelRobertaMLMTrainingDropout():\n",
    "\n",
    "  # Build your model input\n",
    "  inputSentence = Input(shape=(X_LOOKBACK_CNT), name='input', dtype='int32')\n",
    "  \n",
    "  nlp = TFRobertaModel(cfg, name=\"Roberta\")(inputSentence)\n",
    "\n",
    "  categories = Dense(VOCAB_SIZE, activation='softmax', name=\"Categories\")(nlp['last_hidden_state'])\n",
    "  drp = Dropout(0.15, name=\"CategoriesDropout\")(categories)\n",
    "\n",
    "  # categories_weighted = tf.multiply(drp, CLASS_WEIGHTS_TF_CONST)\n",
    "\n",
    "  outputs = [drp]\n",
    "\n",
    "  mnamesuffix = \"_1\"\n",
    "\n",
    "  # And combine it all in a model object\n",
    "  model = Model(inputs=inputSentence, outputs=outputs, name='ModelRobertaMLMTrainingDropout'+mnamesuffix)\n",
    "\n",
    "  return model\n",
    "\n",
    "# model = CreateModelRobertaMLMTrainingDropout()\n",
    "# model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EykwV54JXZUt"
   },
   "outputs": [],
   "source": [
    "#@title CreateModelRobertaLastWordPrediction\n",
    "def CreateModelRobertaLastWordPrediction():\n",
    "\n",
    "  # Build your model input\n",
    "  inputSentence = Input(shape=(X_LOOKBACK_CNT), name='input', dtype='int32')\n",
    "  \n",
    "  nlp = TFRobertaModel(cfg, name=\"Roberta\")(inputSentence)\n",
    "\n",
    "  flat = Flatten(name=\"FlattenNLP\")(nlp['last_hidden_state'])\n",
    "  lastWord = Dense(VOCAB_SIZE, activation='softmax', name=\"Categories\", dtype=tf.float32)(flat)\n",
    "\n",
    "  outputs = [lastWord]\n",
    "\n",
    "  mnamesuffix = \"_1\"\n",
    "\n",
    "  # And combine it all in a model object\n",
    "  model = Model(inputs=inputSentence, outputs=outputs, name='ModelRobertaLastWordPrediction'+mnamesuffix)\n",
    "\n",
    "  return model\n",
    "\n",
    "# model = CreateModelRobertaLastWordPrediction()\n",
    "# model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngVp43WFx4He"
   },
   "outputs": [],
   "source": [
    "# #@title Load the class weights\n",
    "# with file_io.FileIO('gs://ticks_with_indicators_with_volume/sentences/set2/weightArray.npy', mode='rb') as input_f:\n",
    "#   with file_io.FileIO('/content/weightArray.npy', mode='wb+') as output_f:\n",
    "#     output_f.write(input_f.read())\n",
    "\n",
    "# CLASS_WEIGHTS = np.empty((1,1,1024))\n",
    "# CLASS_WEIGHTS[0,0,:] = np.load('/content/weightArray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_lPxR8Fx4ww"
   },
   "outputs": [],
   "source": [
    "# plt.plot(CLASS_WEIGHTS[0,0,:])\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLR-zui53Rj_"
   },
   "outputs": [],
   "source": [
    "# CLASS_WEIGHTS[:] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVv0TNo6x6Do"
   },
   "outputs": [],
   "source": [
    "# CLASS_WEIGHTS_TF_CONST = tf.constant(tf.convert_to_tensor(CLASS_WEIGHTS,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "O7WxTpJPx7t4"
   },
   "outputs": [],
   "source": [
    "#@title CreateModelRobertaMLMTrainingWeighted\n",
    "def CreateModelRobertaMLMTrainingWeighted():\n",
    "\n",
    "  # Build your model input\n",
    "  inputSentence = Input(shape=(X_LOOKBACK_CNT), name='input', dtype='int32')\n",
    "  \n",
    "  nlp = TFRobertaModel(cfg)(inputSentence)\n",
    "\n",
    "  categories = Dense(1024, activation='softmax', name=\"Categories\")(nlp['last_hidden_state'])\n",
    "  categories_weighted = tf.multiply(categories, CLASS_WEIGHTS_TF_CONST)\n",
    "\n",
    "  outputs = [categories_weighted]\n",
    "\n",
    "  mnamesuffix = \"_1\"\n",
    "\n",
    "  # And combine it all in a model object\n",
    "  model = Model(inputs=inputSentence, outputs=outputs, name='ModelRobertaMLMTrainingWeighted'+mnamesuffix)\n",
    "\n",
    "  return model\n",
    "\n",
    "# model = CreateModelRobertaMLMTrainingWeighted()\n",
    "# model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CuRwtJwL8vhY"
   },
   "outputs": [],
   "source": [
    "#@title CreateModelDummy\n",
    "def CreateModelDummy():\n",
    "\n",
    "  # Build your model input\n",
    "  input = Input(shape=(X_LOOKBACK_CNT,FEATURES), name='input', dtype='float32')\n",
    "  \n",
    "  d = Dense(32)(Flatten()(input))\n",
    "  d = Dense(16)(d)\n",
    "  d = Dense(16)(d)\n",
    "  \n",
    "  classifier = Dense(512, activation='relu')(d)\n",
    "  output = Dense(3, activation='softmax')(classifier)\n",
    "  outputs = [ev]\n",
    "\n",
    "  # And combine it all in a model object\n",
    "  model = Model(inputs=input, outputs=outputs, name='RelativeLSTMDenseReduced')\n",
    "\n",
    "  return model\n",
    "# m = CreateModelDummy()\n",
    "# m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz46XBccsTFc"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lirhCyjsH4WE"
   },
   "outputs": [],
   "source": [
    "model = CreateModelRobertaLastWordPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG8wgCoC_-Y9"
   },
   "outputs": [],
   "source": [
    "model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ3vLioK78Iv"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/content/model.h5\"):\n",
    "  copy_filenames = ['gs://ticks_with_indicators_with_volume/chk/TPU_ModelRobertaLastWordPrediction_1_128LB_256VC_FullPrediction/cp_daily_valid_49_end/model.h5'\n",
    "                    ]\n",
    "\n",
    "  for p in copy_filenames:\n",
    "    fn = p.split(\"/\")[-1]\n",
    "\n",
    "    with file_io.FileIO(p, mode='rb') as input_f:\n",
    "      with file_io.FileIO(os.path.join(\"/content\", fn), mode='wb+') as output_f:\n",
    "        output_f.write(input_f.read())\n",
    "        print(\"Pulled from bucket: '\" + fn + \"'\")\n",
    "else:\n",
    "  print(\"DID NOT COPY FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u8BKHrFsNb7"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/content/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGPgeahPsZoW"
   },
   "outputs": [],
   "source": [
    "sentenceMasked, sentence, (XOriginal, yOriginal) = next(tfgenTestMLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-gNzA_vsV-z"
   },
   "outputs": [],
   "source": [
    "p = model.predict(sentenceMasked)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3tmOeUcTDS7"
   },
   "outputs": [],
   "source": [
    "# p_backnormed = np.squeeze(np.nan_to_num((p / CLASS_WEIGHTS), nan=0, posinf=0, neginf=0))\n",
    "# p_backnormed.shape\n",
    "\n",
    "#p_backnormed = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syjwS6mlY8g7"
   },
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maKILo2zZApq"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(p[:,:].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehMqGNfWZeWo"
   },
   "outputs": [],
   "source": [
    "pcopy = copy.deepcopy(p)\n",
    "def func(x):\n",
    "    x[np.argwhere(x != x.max())] = 0\n",
    "    x[x != 0] = 1\n",
    "    x *= np.array(range(256))\n",
    "    return x\n",
    "\n",
    "pmax = np.apply_along_axis(func, 1, pcopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIcdCH9hBwOl"
   },
   "outputs": [],
   "source": [
    "pmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4FXk2YZZpKZ"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(pmax[:,:].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJAn1zS_F8Q3"
   },
   "outputs": [],
   "source": [
    "pSentence = np.sum(pmax, axis=1)\n",
    "pSentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivMtGikoZy8h"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(pSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo4nDJjxQXX1"
   },
   "outputs": [],
   "source": [
    "# for i in range(64):\n",
    "#   print(np.var(pSentence[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubmuP0WNc3pW"
   },
   "outputs": [],
   "source": [
    "showIndex = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R209RD3gc-x4"
   },
   "outputs": [],
   "source": [
    "sentenceMasked[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl2JEBYVc3Hf"
   },
   "outputs": [],
   "source": [
    "sentence[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGw-q4KNGELC"
   },
   "outputs": [],
   "source": [
    "pSentence[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax4j7LX9dD2f"
   },
   "outputs": [],
   "source": [
    "sentenceMasked[showIndex,:] - pSentence[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VLnsJ3IGJ-V"
   },
   "outputs": [],
   "source": [
    "sentence[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0VsFsViBoF8"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(pmax[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cQ-5WmdR2F"
   },
   "source": [
    "## Convert it back to an time-based XFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHcaVVJ9dQlB"
   },
   "outputs": [],
   "source": [
    "XFrameTemplates = np.load(\"/content/drive/MyDrive/Privat/Crypto/XFrames256.npy\")\n",
    "XFrameTemplates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2VcmWZ2daHl"
   },
   "outputs": [],
   "source": [
    "XFrame = np.zeros((pSentence.shape[1],XFrameTemplates.shape[1]))\n",
    "\n",
    "for i, word in enumerate(pSentence[showIndex,:]):\n",
    "  XFrame[i,:] = XFrameTemplates[int(word), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "notVA7XY8Jex"
   },
   "outputs": [],
   "source": [
    "pSentence[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF3ZOy8Y8Oyb"
   },
   "outputs": [],
   "source": [
    "sentence[showIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9xAr12H7w2m"
   },
   "outputs": [],
   "source": [
    "# From source data\n",
    "XFrameGroundTruth = np.zeros((sentence.shape[1],XFrameTemplates.shape[1]))\n",
    "\n",
    "for i, word in enumerate(sentence[showIndex,:]):\n",
    "  XFrameGroundTruth[i,:] = XFrameTemplates[int(word), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIdHv7rNd7FN"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "_ = ax1.plot(XFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqmT_ygAeROh"
   },
   "outputs": [],
   "source": [
    "XFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1RZKKSL8VhQ"
   },
   "outputs": [],
   "source": [
    "XFrameGroundTruth[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aWrkoKBeGWE"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(30,10))\n",
    "_ = ax1.plot(XFrame[:,3])\n",
    "# _ = ax1.plot(XFrameGroundTruth[:,3], color=\"green\")\n",
    "_ = ax1.plot(XOriginal[showIndex,:,3], color=\"green\")\n",
    "\n",
    "# ax1.set_ylim(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnzcIkYR8-O6"
   },
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCcnP4kk9FIe"
   },
   "outputs": [],
   "source": [
    "pSentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RR8QMpcxyLHG"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S59x1d24922y"
   },
   "outputs": [],
   "source": [
    "# Set an optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-04,\n",
    "    epsilon=1e-06,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PGZUQGU-M-d"
   },
   "outputs": [],
   "source": [
    "# CLASS_WEIGHTS = np.empty((1,1,1024))\n",
    "\n",
    "# for i in range(1024):\n",
    "#   CLASS_WEIGHTS[0,0,i] = i * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-R9KPQA91VJ"
   },
   "outputs": [],
   "source": [
    "# Create a masked loss to predict only the missing tokens\n",
    "# https://stackoverflow.com/questions/56328140/how-do-i-implement-a-masked-softmax-cross-entropy-loss-function-in-keras\n",
    "\n",
    "SCCE = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def sparse_crossentropy_masked_weighted(y_true, y_pred):\n",
    "  y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, -1))\n",
    "\n",
    "  y_pred_weighted = tf.multiply(y_pred, CLASS_WEIGHTS)\n",
    "  y_pred_masked = tf.boolean_mask(y_pred_weighted, tf.not_equal(y_true, -1))\n",
    "\n",
    "  return tf.reduce_sum(SCCE(y_true_masked, y_pred_masked)) * (1. / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxwKKPLy939H"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# model.compile(\n",
    "#     optimizer = optimizer,\n",
    "#     loss = [tf.keras.losses.MeanSquaredError()], \n",
    "#     metrics=[tf.keras.losses.MeanAbsoluteError(), tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = sparse_crossentropy_masked_weighted, \n",
    "    metrics=None)\n",
    "# model.compile(\n",
    "#     optimizer = optimizer,\n",
    "#     loss = ['mse'], \n",
    "#     metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0U1e-dr96fs"
   },
   "outputs": [],
   "source": [
    "model.fit(tfgenTrainMLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtxYypqGDT_X"
   },
   "source": [
    "# Calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5Roigp6Dnpw"
   },
   "outputs": [],
   "source": [
    "def genCalcWeights():\n",
    "  it = DataStreamCreator.FileListToDataStream(trainfiles, BATCH_SIZE, TRAIN_PATH,\n",
    "                                              smooth_cnt=SMCNT, smooth_cnt2=SMCNT2,\n",
    "                                              X_lookback_cnt = X_LOOKBACK_CNT,\n",
    "                                              y_lookahead_cnt = Y_LOOKAHEAD_CNT,\n",
    "                                              gain_lookaround_cnt = GAIN_LOOKAROUND_CNT,\n",
    "                                              shortspan=SHORTSPAN, midspan=MIDSPAN, longspan=LONGSPAN,\n",
    "                                              y_type = Y_TYPE,\n",
    "                                              parallel_generators = 8, random_seed=RANDOM_SEED,\n",
    "                                              y_exponent = 1.0,\n",
    "                                              norm_price_related_indicators = False,\n",
    "                                              shuffle=True)\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      ne = next(it)\n",
    "      sentence, sentenceMasked = TransformXFrameToMaskedSentence(ne[0], hideNonMaskedTokensInY=False)\n",
    "\n",
    "      yield (sentenceMasked, sentence)\n",
    "\n",
    "    except StopIteration as si:\n",
    "      logging.warning(\"StopIteration in FileListToDataStream\")\n",
    "      logging.warning(si)\n",
    "      return\n",
    "\n",
    "gcw = genCalcWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3nDAwDODTww"
   },
   "outputs": [],
   "source": [
    "countArray = np.zeros((1024))\n",
    "\n",
    "for ep in tqdm(range(1000)):\n",
    "  maskedSentence, fullSentence = next(gcw)\n",
    "\n",
    "  for w in fullSentence:\n",
    "    countArray[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEUhNtmTERg2"
   },
   "outputs": [],
   "source": [
    "plt.plot(countArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vNC_A9TFNC5"
   },
   "outputs": [],
   "source": [
    "normedArray = countArray / np.sum(countArray)\n",
    "plt.plot(normedArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxbncIhXFxfO"
   },
   "outputs": [],
   "source": [
    "weightArray = np.array(normedArray)\n",
    "\n",
    "maskArray = np.empty(weightArray.shape)\n",
    "maskArray[:] = 1.0\n",
    "maskArray[weightArray == 0.0] = 0.0\n",
    "\n",
    "weightArray = 1.0 / (normedArray)\n",
    "weightArray = np.nan_to_num(weightArray,nan=0,posinf=0,neginf=0)\n",
    "\n",
    "weightArray /= np.sum(weightArray)\n",
    "weightArray[weightArray >= 0.01] = 0.01\n",
    "weightArray /= np.sum(weightArray)\n",
    "\n",
    "plt.plot(weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8qVuZg3JLVI"
   },
   "outputs": [],
   "source": [
    "plt.plot(weightArray)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EW_ERBI4F_uB"
   },
   "outputs": [],
   "source": [
    "np.sum(weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EvArZA4IRq7"
   },
   "outputs": [],
   "source": [
    "plt.plot(countArray * weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vCnnEmVKEW_"
   },
   "outputs": [],
   "source": [
    "# Save the weight array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJXZKN7OKGU8"
   },
   "outputs": [],
   "source": [
    "np.save(\"/content/weightArray.npy\", weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-Bl3NE1KYRA"
   },
   "outputs": [],
   "source": [
    "for fn in [\"/content/weightArray.npy\"]:\n",
    "  fnBucket = fn.replace('/content', 'gs://ticks_with_indicators_with_volume/sentences/set2')\n",
    "\n",
    "  with file_io.FileIO(fn, mode='rb') as input_f:\n",
    "    with file_io.FileIO(fnBucket, mode='wb+') as output_f:\n",
    "      output_f.write(input_f.read())\n",
    "  print(\"Pushed to bucket: '\" + fnBucket + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0KnBnGXK7fo"
   },
   "outputs": [],
   "source": [
    "#@title Load the class weights\n",
    "with file_io.FileIO('gs://ticks_with_indicators_with_volume/sentences/set2/weightArray.npy', mode='rb') as input_f:\n",
    "  with file_io.FileIO('/content/weightArray.npy', mode='wb+') as output_f:\n",
    "    output_f.write(input_f.read())\n",
    "\n",
    "CLASS_WEIGHTS = np.empty((1,1,1024))\n",
    "CLASS_WEIGHTS[0,0,:] = np.load('/content/weightArray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2K8JMEW6kpd"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39-dR_pSIFWt"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YQNg4pMICFU"
   },
   "outputs": [],
   "source": [
    "brc = Birch(n_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9UxCMxOH0a3"
   },
   "outputs": [],
   "source": [
    "def NormForBirch(XIn):\n",
    "  normed = XIn\n",
    "  normed[normed >= 100] = 100\n",
    "  normed[normed <= -100] = -100\n",
    "  normed += 100 + 1\n",
    "  normed = np.log10(normed)\n",
    "  normed -= np.log10(100 + 1)\n",
    "  \n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHKRDJavHxDK"
   },
   "outputs": [],
   "source": [
    "# Load the BIRCH categorizer\n",
    "with open(\"/content/drive/MyDrive/Privat/Crypto/birch.pkl\", 'rb') as pickle_file:\n",
    "  brc = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tR9nAardIIKD"
   },
   "outputs": [],
   "source": [
    "brc.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y45VISnG0rsq"
   },
   "outputs": [],
   "source": [
    "np.save(\"/content/drive/MyDrive/diff.csv\", diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sImGo6izsYEq"
   },
   "outputs": [],
   "source": [
    "#plt.plot(diff[0,:])\n",
    "plt.plot(fullSentence[0,:])\n",
    "plt.plot(maskedSentence[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEB5NUwtsUH9"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXE8AjjoEWE4"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jkOrQ01hZnE"
   },
   "outputs": [],
   "source": [
    "CHKPNT_NAME = \"{}_T{}_{}LB_LF{}_EN{}_{}_{}_EX{}_{}_SM{}_{}\".format(model.name, Y_TYPE, X_LOOKBACK_CNT, EXPECTED_GAIN_LOOKFORWARD, ENTR_THR, ENTR_THR2, ENTR_THR3, EXIT_THR, EXIT_THR2, SMCNT, SMCNT2)\n",
    "CHKPNT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDAS1BhBXJ67"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfwL7EWInGWB"
   },
   "source": [
    "\n",
    "#Pretraining\n",
    "\n",
    "https://huggingface.co/roberta-base\n",
    "\n",
    "The model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The optimizer used is Adam with a learning rate of 6e-4, 1=0.9\\beta_{1} = 0.91=0.9, 2=0.98\\beta_{2} = 0.982=0.98 and =1e6\\epsilon = 1e-6=1e6, a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning rate after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0VmovApkwAB"
   },
   "outputs": [],
   "source": [
    "# Set an optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=1e-06,\n",
    "    epsilon=1e-06,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOLSygN7Ut7_"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# model.compile(\n",
    "#     optimizer = optimizer,\n",
    "#     loss = [tf.keras.losses.MeanSquaredError()], \n",
    "#     metrics=[tf.keras.losses.MeanAbsoluteError(), tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = [tf.keras.losses.SparseCategoricalCrossentropy()], \n",
    "    metrics=None)\n",
    "# model.compile(\n",
    "#     optimizer = optimizer,\n",
    "#     loss = ['mse'], \n",
    "#     metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Imu-PdO8J_0"
   },
   "outputs": [],
   "source": [
    "# # Load the input Conv weights (cannot be trained due to float-->int cast)\n",
    "# copy_filenames = ['gs://ticks_with_indicators_with_volume/chk/MergeFeatureToOneFloat_1_T3_128LB_LF168_EN0.9_0.8_0.1_EX-0.75_-0.7_SM48_48/cp_daily_valid_0_00000/ModelMergeFeatureToOneFloat.h5']\n",
    "\n",
    "# for p in copy_filenames:\n",
    "#   fn = p.split(\"/\")[-1]\n",
    "\n",
    "#   with file_io.FileIO(p, mode='rb') as input_f:\n",
    "#     with file_io.FileIO(os.path.join(\"/content\", fn), mode='wb+') as output_f:\n",
    "#       output_f.write(input_f.read())\n",
    "#       print(\"Pulled from bucket: '\" + fn + \"'\")\n",
    "\n",
    "# mInit = CreateModelMergeFeatureToOneFloat()\n",
    "\n",
    "# # # Saved model to: 'gs://ticks_with_indicators_with_volume/chk/MergeFeatureToOneFloat_1_T3_128LB_LF168_EN0.9_0.8_0.1_EX-0.75_-0.7_SM48_48/cp_daily_valid_0_00000/ModelMergeFeatureToOneFloat.h5'\n",
    "# mInit.load_weights('/content/ModelMergeFeatureToOneFloat.h5')\n",
    "\n",
    "# # model.get_layer(\"MergeFeatureToOneFloat\").set_weights(\n",
    "# #     model.get_layer(\"MergeFeatureToOneFloat\").get_weights()\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL-VAe80hcnH"
   },
   "outputs": [],
   "source": [
    "# mInit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuuH0xBKGE1g"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC5mKfMt5z2J"
   },
   "outputs": [],
   "source": [
    "# # !gcloud config set project {project_id} -y\n",
    "# !gsutil cp -R gs://ticks_with_indicators_with_volume/logs/ModelNLP_1_T3_128LB_LF168_EN0.9_0.8_0.1_EX-0.75_-0.7_SM48_4820220915-162219 /content/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOJpZbR84tTr"
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir /content/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysPZj5WZ4qSs"
   },
   "outputs": [],
   "source": [
    "# from tensorboard import notebook\n",
    "# notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFa0iWcaqli5"
   },
   "outputs": [],
   "source": [
    "model.summary(line_length=220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGC1RvizroRS"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, save_freq, val_freq, checkpoint_path, model_name, epoch_add=0):\n",
    "    self.save_freq = save_freq\n",
    "    self.val_freq = val_freq\n",
    "    self.checkpoint_path = checkpoint_path\n",
    "    self.model_name = model_name\n",
    "    self.current_epoch = 0\n",
    "    self.epoch_add = epoch_add\n",
    "\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    self.current_epoch = epoch + self.epoch_add\n",
    "    # keys = list(logs.keys())\n",
    "    # print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "  def on_train_epoch_end(self, logs=None):\n",
    "    self.saveTheModel(99999,logs)\n",
    "\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    self.saveTheModel(batch,logs)\n",
    "\n",
    "  def saveTheModel(self, batch, logs=None):\n",
    "    if 0 < batch and 0 == batch % self.save_freq:\n",
    "      _save_folder = os.path.join(self.checkpoint_path,\n",
    "                                self.model_name,\n",
    "                                \"cp_daily_valid_{:01d}_{:05d}\".format(self.current_epoch, batch)\n",
    "                                )\n",
    "\n",
    "      _model_path_local = os.path.join(\"/content/\", \"model.h5\")\n",
    "      _model_path_bucket = os.path.join(_save_folder, \"model.h5\")\n",
    "\n",
    "      model.save(_model_path_local)\n",
    "     \n",
    "      # Copy model.h5 over to Google Cloud Storage\n",
    "      with file_io.FileIO(_model_path_local, mode='rb') as input_f:\n",
    "          with file_io.FileIO(_model_path_bucket, mode='wb+') as output_f:\n",
    "              output_f.write(input_f.read())\n",
    "              print(\"\\nSaved model to: '\" + _model_path_bucket + \"'\")\n",
    "\n",
    "      # Save optimizer config\n",
    "      c = copy.deepcopy(self.model.optimizer.get_config())\n",
    "\n",
    "      fp = os.path.join(_save_folder, \"c.pickle\")\n",
    "      with file_io.FileIO(fp, mode='wb+') as handle:\n",
    "        pickle.dump(c, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Saved optimizer config to: '\" + fp + \"'\")\n",
    "\n",
    "      # Save optimizer weights\n",
    "      w = copy.deepcopy(self.model.optimizer.get_weights())\n",
    "\n",
    "      fp = os.path.join(_save_folder, \"w.pickle\")\n",
    "      with file_io.FileIO(fp, mode='wb+') as handle:\n",
    "        pickle.dump(w, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Saved optimizer weights to: '\" + fp + \"'\")\n",
    "\n",
    "    if 0 < batch and 0 == batch % self.val_freq:\n",
    "      print(\"-------------------------EVAL-------------------------\")\n",
    "      model.evaluate(tfgenTestMLM)\n",
    "      print(\"\\n-------------------------EVAL-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyggHiDGNTEX"
   },
   "outputs": [],
   "source": [
    "epoch_add = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pJ08IBN7cgB"
   },
   "outputs": [],
   "source": [
    "CALLBACK_EVERY_N_BATCHES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuGT0rFvzH1n"
   },
   "outputs": [],
   "source": [
    "cc = CustomCallback(checkpoint_path = checkpoint_path,\n",
    "                    model_name = CHKPNT_NAME,\n",
    "                    save_freq = CALLBACK_EVERY_N_BATCHES,\n",
    "                    val_freq = CALLBACK_EVERY_N_BATCHES,\n",
    "                    epoch_add = epoch_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGTsaO_KGH58"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"gs://ticks_with_indicators_with_volume/logs/\" + CHKPNT_NAME + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "     histogram_freq=0,\n",
    "     update_freq=CALLBACK_EVERY_N_BATCHES \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Nh12BlMEOBF"
   },
   "outputs": [],
   "source": [
    "# copy_filenames = ['gs://ticks_with_indicators_with_volume/chk/ModelNLP_1_T3_128LB_LF168_EN0.9_0.8_0.1_EX-0.75_-0.7_SM48_48/cp_daily_valid_10_16000/model.h5'\n",
    "#                   ]\n",
    "\n",
    "# for p in copy_filenames:\n",
    "#   fn = p.split(\"/\")[-1]\n",
    "\n",
    "#   with file_io.FileIO(p, mode='rb') as input_f:\n",
    "#     with file_io.FileIO(os.path.join(\"/content\", fn), mode='wb+') as output_f:\n",
    "#       output_f.write(input_f.read())\n",
    "#       print(\"Pulled from bucket: '\" + fn + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYu7qfd3IsMu"
   },
   "outputs": [],
   "source": [
    "# model.load_weights(\"/content/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhqJUyGUm92l"
   },
   "outputs": [],
   "source": [
    "# modelFirstLayers = CreateModelViTPara4TimeAndFeatureTo2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40Oyn0RqF5Vp"
   },
   "outputs": [],
   "source": [
    "# modelFirstLayers.load_weights(\"/content/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnX52uiZnRmZ"
   },
   "outputs": [],
   "source": [
    "# for l in modelFirstLayers.layers:\n",
    "#   if 0 < len(l.get_weights()):\n",
    "#     lname = l.name\n",
    "#     lweights = l.get_weights()\n",
    "\n",
    "#     # Bugfix:\n",
    "#     if \"tf_vi_t_for_image_classification_1\" == lname:\n",
    "#       lname = \"tf_vi_t_for_image_classification\"\n",
    "\n",
    "#     try:\n",
    "#       model.get_layer(lname).set_weights(lweights)\n",
    "#       # model.get_layer(lname).trainable = False\n",
    "\n",
    "#       print(\"Applied \" + l.name)\n",
    "#     except ValueError as ve:\n",
    "#       print(\"Error\")\n",
    "#       print(ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mV2OpQAGDja"
   },
   "outputs": [],
   "source": [
    "# with open(\"/content/w.pickle\", 'rb') as pickle_file:\n",
    "#   w = pickle.load(pickle_file)\n",
    "# model.optimizer.set_weights(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f37AgOdQSsSY"
   },
   "outputs": [],
   "source": [
    "# class_weights = {0:2.0, 1: 2.0, 2: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXHVjhsDHHF9"
   },
   "outputs": [],
   "source": [
    "# model.optimizer.learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtTR6r4VFCt4"
   },
   "outputs": [],
   "source": [
    "model.fit(tfgenTrainMLM, epochs=200, verbose = 1, callbacks=[tensorboard_callback, cc], validation_data=tfgenTestMLM, validation_freq=500, class_weight=CLASS_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zWTGYiIWScY"
   },
   "outputs": [],
   "source": [
    "p = model.predict(tfgenTest)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVAb1cmV3pFL"
   },
   "outputs": [],
   "source": [
    "p[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuNubhiE3q7v"
   },
   "outputs": [],
   "source": [
    "plt.plot(p[1][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq4a_q4_WfIa"
   },
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4E3uAgQgzYIX",
    "NL2h_bVUyODm",
    "jYQ2HFATz1tA",
    "fn5zpFday4O2",
    "ziS7-tXmy8uU",
    "7TBE84-7oj_I",
    "UZaWazh6osch",
    "oz46XBccsTFc",
    "0_cQ-5WmdR2F",
    "VtxYypqGDT_X",
    "kXE8AjjoEWE4",
    "WDAS1BhBXJ67",
    "QfwL7EWInGWB"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "224c8b2e1f718756ae9bc5c4f62786756774c1d0fb85b85781d494c44704ff9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
